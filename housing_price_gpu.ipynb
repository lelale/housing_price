{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "#5\n",
    "train = pd.read_csv(\"data/kaggle_house_pred_train.csv\")\n",
    "test = pd.read_csv(\"data/kaggle_house_pred_test.csv\")\n",
    "all_X = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'], test.loc[:, 'MSSubClass':'SaleCondition'])) ###dataFrame\n",
    "numeric_feats = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "all_X[numeric_feats] = all_X[numeric_feats].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "all_X = pd.get_dummies(all_X, dummy_na=True)\n",
    "all_X = all_X.fillna(all_X.mean())\n",
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_train = train.SalePrice.as_matrix()\n",
    "\n",
    "X_train = nd.array(X_train, ctx = mx.gpu())\n",
    "y_train = nd.array(y_train, ctx = mx.gpu())\n",
    "y_train = nd.log(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "X_test = nd.array(X_test, ctx = mx.gpu())\n",
    "\n",
    "square_loss = gluon.loss.L2Loss()\n",
    "\n",
    "def get_rmse_log(net, X_train, y_train):\n",
    "    num_train = X_train.shape[0]\n",
    "    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n",
    "    return np.sqrt(2 * nd.sum(square_loss(nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)\n",
    "\n",
    "drop_prob1 = 0.2\n",
    "drop_prob2 = 0.5    \n",
    "\n",
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(256,activation = 'relu'))\n",
    "        net.add(gluon.nn.Dropout(drop_prob1))\n",
    "        net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dropout(drop_prob2))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.initialize(ctx = mx.gpu())\n",
    "    return net\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(net, X_train, y_train, X_test, y_test, epochs, verbose_epoch, learning_rate, weight_decay):\n",
    "    train_loss = []\n",
    "    if X_test is not None:\n",
    "        test_loss = []\n",
    "    batch_size = 64\n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = gluon.data.DataLoader(\n",
    "        dataset_train, batch_size,shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate,\n",
    "                             'wd': weight_decay})\n",
    "    net.collect_params().initialize(force_reinit=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cur_train_loss = get_rmse_log(net, X_train, y_train)\n",
    "\n",
    "        if epoch > verbose_epoch:\n",
    "            print(\"Epoch %d, train loss: %f\" % (epoch, cur_train_loss))\n",
    "        train_loss.append(cur_train_loss)\n",
    "        if X_test is not None:\n",
    "            cur_test_loss = get_rmse_log(net, X_test, y_test)\n",
    "            test_loss.append(cur_test_loss)\n",
    "    plt.plot(train_loss[10:])\n",
    "    plt.legend(['train'])\n",
    "    if X_test is not None:\n",
    "        plt.plot(test_loss[10:])\n",
    "        plt.legend(['train','test'])\n",
    "    plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss\n",
    "    else:\n",
    "        return cur_train_loss\n",
    "    \n",
    "def k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,learning_rate, weight_decay):\n",
    "    assert k > 1\n",
    "    fold_size = X_train.shape[0] // k\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    for test_i in range(k):\n",
    "        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n",
    "        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n",
    "\n",
    "        val_train_defined = False\n",
    "        for i in range(k):\n",
    "            if i != test_i:\n",
    "                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n",
    "                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n",
    "                if not val_train_defined:\n",
    "                    X_val_train = X_cur_fold\n",
    "                    y_val_train = y_cur_fold\n",
    "                    val_train_defined = True\n",
    "                else:\n",
    "                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n",
    "                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n",
    "        net = get_net()\n",
    "        train_loss, test_loss = train(net, X_val_train, y_val_train, X_val_test, y_val_test, epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        train_loss_sum += train_loss\n",
    "        print(\"Test loss: %f\" % test_loss)\n",
    "        test_loss_sum += test_loss\n",
    "    return train_loss_sum / k, test_loss_sum / k\n",
    "\n",
    "k = 5\n",
    "epochs = 40\n",
    "verbose_epoch = 35\n",
    "learning_rate = 0.02\n",
    "weight_decay = 0.8\n",
    "\n",
    "train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,y_train, learning_rate, weight_decay)\n",
    "print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" % (k, train_loss, test_loss))\n",
    "\n",
    "def learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "          weight_decay):\n",
    "    net = get_net()\n",
    "    train(net, X_train, y_train, None, None, epochs, verbose_epoch,\n",
    "          learning_rate, weight_decay)\n",
    "    preds = net(X_test).asnumpy()\n",
    "    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n",
    "    name = \"submission\"+\"256_128\"+str(learning_rate)+str(weight_decay)+str(drop_prob1)+str(drop_prob2) +\".csv\"\n",
    "    submission.to_csv(name, index=False)\n",
    "    \n",
    "learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate, weight_decay)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
